[{"categories":["Deep Learning"],"content":"Reference Yang X, Song Z, King I, et al. A Survey on Deep Semi-supervised Learning[J]. arXiv preprint arXiv:2103.00550, 2021. ","date":"2021-11-05","objectID":"/semisupervisedlearning/:1:0","tags":[],"title":"Deep Semi-supervised Learning [A Survey]","uri":"/semisupervisedlearning/"},{"categories":["Deep Learning"],"content":"概述 本篇综述不涉及关于SSL综述代表作的内容，而是主要针对基于DL的算法。 ","date":"2021-11-05","objectID":"/semisupervisedlearning/:2:0","tags":[],"title":"Deep Semi-supervised Learning [A Survey]","uri":"/semisupervisedlearning/"},{"categories":["Deep Learning"],"content":"SSL任务的分类 Semi-supervised classification Semi-supervised clustering Semi-supervised regression ","date":"2021-11-05","objectID":"/semisupervisedlearning/:2:1","tags":[],"title":"Deep Semi-supervised Learning [A Survey]","uri":"/semisupervisedlearning/"},{"categories":["Deep Learning"],"content":"DSSL模型的分类 Generative methods Consistency regularization methods Graph-based methods Pseudo-labeling methods hybrid methods DSSL分类DSSL分类 \"\rDSSL分类\r ","date":"2021-11-05","objectID":"/semisupervisedlearning/:2:2","tags":[],"title":"Deep Semi-supervised Learning [A Survey]","uri":"/semisupervisedlearning/"},{"categories":["Deep Learning"],"content":"背景介绍 数据集表示为 $X={X_L, X_U}$ ，其中 $X_L=\\{x_i\\}^L_{i=1}$ 是一个比较小的有标注的子集，标注为 $Y_L=(y_1, y_2,\\dots,y_L)$ ，而 $X_U=\\{x_i\\}^U_{i=1}$ 是一个比较大的无标注的子集，通常假设 $L \\ll U$ 。 假设数据集总共包含 $K$ 个类别，则 $X_L$ 被标注为 $\\{y_i\\}^L_{i=1}\\in(y^1,y^2,\\dots,y^K)$ ，则SSL需要去求解如下的优化问题： $$ \\min _{\\theta} \\underbrace{\\sum_{x \\in X_{L}, y \\in Y_{L}} \\mathcal{L}_{s}(x, y, \\theta)}_{\\text {supervised loss }}+\\alpha \\underbrace{\\sum_{x \\in X_{U}} \\mathcal{L}_{u}(x, \\theta)}_{\\text {unsupervised loss }}+\\beta \\underbrace{\\sum_{x \\in X} \\mathcal{R}(x, \\theta)}_{\\text {regularization }} $$ $\\mathcal{L}_s$ 表示每个样本的监督损失 $\\mathcal{L}_u$ 表示每个样本的无监督损失 $\\mathcal{R}$ 表示每个样本的正则化项（一致性损失或者是设计出的regularization term） 注意：$\\mathcal{L}_u$ 和 $\\mathcal{R}$ 通常没有严格区分，因为 $\\mathcal{R}$ 一般也是与标注信息无关的 根据测试集数据是否提供将SSL分为两种setting： Transductive learning (Graph-based methods) Inductive learning (else methods) ","date":"2021-11-05","objectID":"/semisupervisedlearning/:3:0","tags":[],"title":"Deep Semi-supervised Learning [A Survey]","uri":"/semisupervisedlearning/"},{"categories":["Deep Learning"],"content":"SSL的相关假设 Self-training assumption：Self-training model的预测通常是正确的，因为该假设成立的话，high-confidence predictions就可以被当作真实标签。这个假设通常在类分离的比较好的时候满足。 Co-training assumption：不同合理的假设导致不同的有标签数据和无标签数据的组合。Blum提出一种co-training model，模型满足“实例 $x$ 有两个条件独立的 views ，每个 view 满足一个分类任务”。 Generative model assumption：已知先验 $p(y)$ 和条件分布 $p(x|y)$ 的情况下，可以通过 $p(x,y)=p(y)p(x|y)$ 将无标签数据和类别有效连接起来。 Clustering assumption：同类样本在高维空间中通常成簇聚集，同类的两个样本之间用短线连接通常不会穿过低密度区域，因此大量的无标签数据可以用来调整分类边界。 Low-density separation：与Clustering assumption类似，决策边界不应穿过高密度区域。 Manifold assumption：如果 $x_1$ 和 $x_2$ 在低维流形上局部相邻，他们将拥有相同的类别。这反映的是决策边界的局部平滑性。 ","date":"2021-11-05","objectID":"/semisupervisedlearning/:3:1","tags":[],"title":"Deep Semi-supervised Learning [A Survey]","uri":"/semisupervisedlearning/"},{"categories":["Deep Learning"],"content":"Classical methods 1970年，SSL概念首先被提出，实现思路是采用 self-learning 的方式，先拿有标签的数据训练模型，再拿无标签的数据测试，prediction 值较高的认为是该样本的 ground truth label (pseudo label)，把这部分再拿进模型训练，逐步迭代。 生成模型依据假设 $p(x,y)=p(y)p(x|y)$ ，对于有标签数据， $p(y)$ 和 $p(x,y)$ 已知，需要求解 $p(x|y)$ 的参数，$p(x|y)$ 通常为可识别的分布（多项式分布、高斯分布）。通常用 EM 算法迭代求解。 但可识别的分布通常不能完全匹配真实分布，会导致分类性能下降。 TSVMs (Transductive Support Vector Machines)：和SVMs同样，TSVMs优化数据点和决策边界之间的gap，然后借助无标签数据的信息expand the gap。 Graph-based methods 依赖于有标签数据和无标签数据分布的几何结构（流形）。通过探索数据的图或流形结构，可以借助非常少的标签 propagate information 来学习。（如：Label propagation） ","date":"2021-11-05","objectID":"/semisupervisedlearning/:3:2","tags":[],"title":"Deep Semi-supervised Learning [A Survey]","uri":"/semisupervisedlearning/"},{"categories":["Deep Learning"],"content":"相关学习范式 迁移学习：Transfer Learning 弱监督学习：Weakly-supervised Learning Positive and unlabeled learning 元学习：Meta Learning 自监督学习：Self-supervised Learning ","date":"2021-11-05","objectID":"/semisupervisedlearning/:3:3","tags":[],"title":"Deep Semi-supervised Learning [A Survey]","uri":"/semisupervisedlearning/"},{"categories":["Deep Learning"],"content":"数据集和应用 相关数据集和应用相关数据集和应用 \"\r相关数据集和应用\r ","date":"2021-11-05","objectID":"/semisupervisedlearning/:3:4","tags":[],"title":"Deep Semi-supervised Learning [A Survey]","uri":"/semisupervisedlearning/"},{"categories":["Deep Learning"],"content":"Generative Methods ","date":"2021-11-05","objectID":"/semisupervisedlearning/:4:0","tags":[],"title":"Deep Semi-supervised Learning [A Survey]","uri":"/semisupervisedlearning/"},{"categories":["Deep Learning"],"content":"Semi-supervised GANs GAN模型的演化关系GAN模型的演化关系 \"\rGAN模型的演化关系\r 众所周知，GAN的损失函数表示如下： $$ \\min _{G} \\max _{D} V(D, G) = \\mathbb{E} _{x \\sim p(x)} [\\log D(x)] + \\mathbb{E} _{z \\sim p _{z}} [\\log (1-D(G(z)))] $$ GANGAN \"\rGAN\r 因为GAN可以从无标签数据中学习数据的真实分布，所以它可以用于SSL任务，而具体的应用方式分为如下四种： 重用（re-use） Discriminator 的特征 使用 GAN 生成的样本去正则化（regularize）分类器 学习一个 inference model 使用 GAN 生成的样本作为另外的训练数据 CatGAN Categorical Generative Adversarial Network CatGANCatGAN \"\rCatGAN\r 把 GAN 的 Discriminator 改成了分类器（而不是仅判断生成的样本是否真实）。个人理解主要目的有两个： 可以实现无监督的聚类，因为生成的样本会被判别器主动分类，只不过类别个数需要手动设定； 可以实现半监督学习，利用有标签数据同时训练模型，因为 Discriminator 可以输出分类。 Loss components\r\r $H[p(y \\mid x, D)]$ ：对于无标签数据，最小化条件概率的熵，即 $D$ 需要对 $x$ 给出更明确地类别输出； $H[p(y \\mid G(z), D)]$ ：对于生成的数据，最大化条件概率的熵，即 $D$ 需要对 $G(z)$ 给出近似均匀分布的输出（GAN的思想）； $H[p(y\\mid D)]$ ：最大化类别分布先验的熵，是在各类样本个数基本相等的假设下，保证 $D$ 对样本的聚类没有偏见。这里是对数据集中所有样本去计算的，所以无法与前面的两个损失在一个 batch 里面兼容，文中对它的计算做了特殊处理。 $C E[\\mathbf{y}, p(y \\mid \\mathbf{x}, D)]$ ：有标签数据的交叉熵，用于半监督学习的设定。 注意：这里提到的损失函数最大或者最小化要根据模型训练在 $G$ step 还是在 $D$ step 来确定，满足GAN的训练规则。 \r\r CCGAN Context-Conditional Generative Adversarial Network CCGANCCGAN \"\rCCGAN\r 主要亮点是利用图像周围的像素（context information）学习图像特征。图中的 $m$ 是一个二值的 mask，用于 drop out 图像中的 specific portion （比如一个方形区域）。$x_{I}=(1-m) \\odot x_{G}+m \\odot x$ 表示 in-painted image，就是根据输入的挖空图像生成的补全图像。其中 $x_{G}=G(m \\odot x, z)$ 表示根据挖空图像和噪声生成的图像。其余部分与 GAN 相同，个人觉得类似 Transformer 中的掩码机制。 Improved GAN Improved Techniques for Training GANs Improved GANImproved GAN \"\rImproved GAN\r 与 CatGAN 相比，ImprovedGAN 在判别器 $D$ 的输出上多加了一类，即第 $K+1$ 类，表示 $G$ 生成的数据。直观理解就是，$D$ 本来是要判别样本是 real sample 还是 generated sample 的，但是 CatGAN 把它改成了分类器，即默认 generated sample 一定属于 $K$ 类。这显然是有问题的，如果生成的数据压根啥都不是呢，所以给 $D$ 多加一类，强制 $G$ 要生成与真实样本相似的前 $K$ 类的数据。此外，这篇文章还提出了很多训练 GAN 的技巧，这里做一下列举，详细的请参考原文。 Techniques\r\r Feature matching Minibatch discrimination Historical averaging (这个很像自监督学习里的MoCo方法) One-sided label smoothing Virtual batch normalization \r\r GoodBadGAN Good semi-supervised learning that requires a bad GAN GoodBadGANGoodBadGAN \"\rGoodBadGAN\r 这篇文章意识到 Improved GAN 的 $G$ 和 $D$ 可能不能同时达到最优（即判别器达到了最优效果，但是生成器可能产生不真实的样本）。该方法从理论上证明了，为什么 bad samples 可以增强 SSL 的 performance。这里给出一个直观解释，理论的证明参考原文。 对于一个 Perfect Generator，训练的目标是期望它与真实数据的分布一致：$p_{G}=p$ 。但是如此一来会导致 $D$ 的最优解等价于有监督损失的最优解（原文中有理论证明），即无标签的数据失效了。而一个 Complete Generator，是应该能够产生 Bad samples 的，因为这样可以使 $G$ 生成的部分 Bad sample 填充高维空间中的低密度区域，使得 $D$ 的 boundary 不会落在类内的低密度区域，避免分界面穿过流形。 Loss components\r\r $\\mathbb{E} _{x \\sim p _G} \\log p(x) \\mathbb{I}[p(x)\u003e\\epsilon]$ ：$G$ 的优化目标（最小化），用于生成 bad samples 的惩罚项，它只对高密度区域的样本起作用，低密度区域的样本不受影响（$\\mathbb{I}[\\cdot]$ 是示性函数）。这里是指对于 $p(x)\u003e\\epsilon$ 的样本，约束使其 $p(x)$ 越小越好，直至 $p(x)\u003c\\epsilon$ 后即可消除惩罚（该样本的惩罚等于0） $\\mathbb{E} _{x \\sim \\mathcal{U}} \\sum _{k=1}^{K} p _{D}(k \\mid x) \\log p _{D}(k \\mid x)$ ：$D$ 的优化目标（最大化：这里是负的条件熵，相当于最小化条件熵），用于使无标签数据的输出尽可能地确定，避免 $D$ 对于无标签数据的分类结果过于均匀。 \r\r Localized GAN Global versus Localized Generative Adversarial Nets Localized GANLocalized GAN \"\rLocalized GAN\r 通常的 GAN 是指用生成器 $G$ 从随机噪声 $z\\in\\mathbb{R}^N$ 生成样本 $G(z)\\in \\mathbb{R}^D$ ，在这种情况下，生成样本 $G(z)$ 的环境空间（ambient space）是用 $N$ 维的全局坐标系 $z$ 来表示的。所有生成的样本会形成一个 $N$ 维的 manifold $\\mathcal{M} = \\{ G(\\mathbf{z}) \\mid \\mathbf{z} \\in \\mathbb{R}^{N} \\}$ 。这样的假设有两个缺陷： 问题\r\r 在全局坐标系的假设下，样本点 $x$ 的局部结构无法直接得到，因为流形空间是 $N$ 维的，$D$ 维的样本空间只是一个 Embedding，所以必须知道 $G^{-1}(\\cdot)$ 才能从 $x$ 映射回 $z$ 从而知道 $x$ 的局部结构。 如果 $x$ 的维数有缺陷（$\\mathcal{T} _x \u003c N $），则切空间 $\\mathcal{T} _{x}$ 会产生局部塌陷（local collapse）。如此一来，当 $z$ 在某些方向上发生改变时，$G(z)$ 就不会再产生有意义的数据点（即 $x$ 不再变化）。 \r\r 所以，本文提出 local generator $G(x,z)$ 满足如下两个条件： Conditions\r\r locality ：$G(x, 0)=x$ , i.e., 局部坐标 $z$ 的原点必须在 $x$ orthonormality ：$\\mathbf{J} _{x} ^{T} \\mathbf{J} _{x}=\\mathbf{I} _{N}$ , i.e., $\\mathcal{T}_x$ 的基必须是标准正交的，保证 $x$ 的局部不会产生塌陷。 \r\r 通过最小化如下的正则项来约束这两个条件： $$ \\Omega_{G}(\\mathbf{x})=\\mu|G(\\mathbf{x}, \\mathbf{0})-\\mathbf{x}|^{2}+\\eta\\left|\\mathbf{J}_{\\mathbf{x}}^{T} \\mathbf{J}_{\\mathbf{x}}-\\mathbf{I}_{N}\\right|^{2} $$ 本文通过解决流形上函数的求导问题（详情见论文），在半监督学习任务上得到了一个局部一致的分类器。 CT-GAN CT-GANCT-GAN \"\rCT-GAN\r 结合 Consistency training 和 WGAN 用于半监督学习，依赖于 Lipschitz 连续性条件。一致性约束通过对样本添加两次扰动来实现","date":"2021-11-05","objectID":"/semisupervisedlearning/:4:1","tags":[],"title":"Deep Semi-supervised Learning [A Survey]","uri":"/semisupervisedlearning/"},{"categories":["Deep Learning"],"content":"Semi-supervised VAE 变分自动编码器（Variational AutoEncoders）使用估计的后验分布 $q(z|x)$ 来代替真实的后验分布 $p(z|x)$ 。其置信下界ELBO（Evidence lower bound）写为： $$ \\log p(x) \\geq \\log \\mathbb{E} _{q(z \\mid x)}\\left[\\frac{p(z) p(x \\mid z)}{q(z \\mid x)}\\right] = \\mathbb{E} _{q(z \\mid x)}[\\log p(z) p(x \\mid z)-\\log q(z \\mid x)] $$ 为什么隐变量模型可以用于SSL？\r\r 这是一种引入未标记数据的自然方式 通过隐变量的设置，可以轻松实现分离表征（表征解耦）的能力 可以使用变分方法 \r\r SSVAEs 一个具有隐编码表示的基于VAE的生成模型。其中介绍了3个模型： Latent-feature discriminative model (M1)： M1M1 \"\rM1\r 即最普通的 VAE 模型，用深度网络构建 $p_{\\theta}(x|z)$ 和 $q_{\\phi}(z|x)$ 使用隐变量 $z$ 表示图像特征，用于后续的分类。 Generative semi-supervised model (M2)： M2M2 \"\rM2\r 相当于Conditional-VAE，将类别 $y$ 加入隐变量，$y$ 服从多项式分布 $p(y)=\\mathop{Cat}(y|\\boldsymbol{\\pi})$ ，从而在生成样本时加入类别信息，生成器为 $p_{\\theta}(x|y,z)$ 。对于无标签的数据，可以根据后验分布 $p_{\\theta}(y|x)$ 对其标签进行推理。 Stacked generative semi-supervised model (M1+M2)： M1+M2M1+M2 \"\rM1+M2\r 使用 M1 中的 $z$ 作为 M2 中的生成目标 $x$ ，即用 M2 中的 $z_2$ 和 $y$ 生成 $x$ 的隐变量特征表示 $z_1$ ，再用其隐变量 $z_1$ 生成 $x$ 。 $$ p_{\\theta}(x,y,z_1,z_2)=p_{\\theta}(x|z_1)p_{\\theta}(z_1|y,z_2)p(y)p(z_2) $$ 其中 $p_{\\theta}(x|z_1)$ 和 $p_{\\theta}(z_1|y,z_2)$ 用神经网络建模。 对于有标签的样本，ELBO为： $$ \\log p_{\\theta}(\\mathbf{x}, y) \\geq \\mathbb{E}_{q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}, y)}\\left[\\log p_{\\theta}(\\mathbf{x} \\mid y, \\mathbf{z})+\\log p_{\\theta}(y)+\\log p(\\mathbf{z})-\\log q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}, y)\\right]=-\\mathcal{L}(\\mathbf{x}, y) $$ 对于无标签的样本，ELBO为： $$ \\begin{aligned} \\log p_{\\theta}(\\mathbf{x}) \u0026 \\geq \\mathbb{E}_{q_{\\phi}(y, \\mathbf{z} \\mid \\mathbf{x})}\\left[\\log p_{\\theta}(\\mathbf{x} \\mid y, \\mathbf{z})+\\log p_{\\theta}(y)+\\log p(\\mathbf{z})-\\log q_{\\phi}(y, \\mathbf{z} \\mid \\mathbf{x})\\right] \\\\ \u0026=\\sum_{y} q_{\\phi}(y \\mid \\mathbf{x})(-\\mathcal{L}(\\mathbf{x}, y))+\\mathcal{H}\\left(q_{\\phi}(y \\mid \\mathbf{x})\\right)=-\\mathcal{U}(\\mathbf{x}) \\end{aligned} $$ 最终整个数据集的损失函数为： $$ \\mathcal{J}=\\sum_{(\\mathbf{x}, y) \\sim \\widetilde{p}_{l}} \\mathcal{L}(\\mathbf{x}, y)+\\sum_{\\mathbf{x} \\sim \\widetilde{p}_{u}} \\mathcal{U}(\\mathbf{x}) $$ ADGM Auxiliary Deep Generative Models 是 SSVAEs 的一种扩展形式，加入了辅助变量 $a$： Auxiliary deep generative modelAuxiliary deep generative model \"\rAuxiliary deep generative model\r 他解决的关键问题是：$q(z|x)$ 通常被定义为 diagonal Gaussian 的分布，这限制了模型的表达能力。 加入辅助变量 $a$ 使得 $p(x,z)$ 变为 $p(x,z,a)=p(a|x,z)p(x,z)$ ，从而让变分分布 $q(z|x)=\\int q(z|a,x)p(a|x)$ 变为一个一般的非高斯分布，以应对更复杂的后验分布 $p(z|x)$ ，提升模型的推断能力。 添加了 auxiliary variable 的 lower bound 变为： $$ \\log p_{\\theta} (x) \\geq \\mathbb{E}_{q_{\\phi}(a, z \\mid x)}\\left[ \\log \\frac{p(x,z,a)}{q(z|x)} \\right] = \\mathbb{E}_{q_{\\phi}(a, z \\mid x)}\\left[\\log \\frac{p_{\\theta}(a \\mid z, x) p_{\\theta}(x \\mid z) p(z)}{q_{\\phi}(a \\mid x) q_{\\phi}(z \\mid a, x)}\\right] \\equiv-\\mathcal{U}_{\\mathrm{AVAE}}(x) $$ 其中 $p_{\\theta}(a|z,x)$ 和 $q_{\\phi}(a|x)$ 用神经网络建模为 diagonal Gaussian 分布。 Infinite VAE Infinite VAE 提出一种 VAE 的混合模型（一个非参数化的贝叶斯方法）。 这篇没有详细看，涉及到 Dirichlet process，Gibbs sampling 和 variational inference，以后有时间再补充 Infinite VAEInfinite VAE \"\rInfinite VAE\r Disentangled VAE ","date":"2021-11-05","objectID":"/semisupervisedlearning/:4:2","tags":[],"title":"Deep Semi-supervised Learning [A Survey]","uri":"/semisupervisedlearning/"},{"categories":["Paper"],"content":"动词短语 come to the fore 脱颖而出 play a dominating role 很重要 take into account sth. 考虑在内 reside in/on 坐落在、居住在 ","date":"2021-11-05","objectID":"/writing/:1:0","tags":["Writing","Paper"],"title":"For English Writing","uri":"/writing/"},{"categories":["Paper"],"content":"名词短语 essential prerequisite 基本前提 ","date":"2021-11-05","objectID":"/writing/:2:0","tags":["Writing","Paper"],"title":"For English Writing","uri":"/writing/"},{"categories":["Paper"],"content":"状语短语 motivated by these observations 受这些观察的启发 More formally, ... 字面义：更正式地；表示“更详细地、更严格地”，后面跟方法的详细描述或者公式。 The method gives theoretical justifications of ... 该方法从理论上证明了… When it comes to ... 当谈到…时 ","date":"2021-11-05","objectID":"/writing/:3:0","tags":["Writing","Paper"],"title":"For English Writing","uri":"/writing/"},{"categories":["Paper"],"content":"转折词(连接词) Regarding (whether) ... 根据（是否）… ","date":"2021-11-05","objectID":"/writing/:4:0","tags":["Writing","Paper"],"title":"For English Writing","uri":"/writing/"},{"categories":["Methods","Code"],"content":"写在前面：最近几天新冠病毒疫情还未平息，在家帮女票研究波浪模型的时候探索了一下用Matlab进行波浪数值模拟的简单方法，在这里写一个简单教程，因为我在网上也没有找到写的比较完整的波浪模拟代码，所以来这里占个坑，希望对大家有所帮助。 ","date":"2020-02-17","objectID":"/matlabwavesimulation/:0:0","tags":["船舶与海洋工程","数值模拟"],"title":"Matlab波浪数值模拟","uri":"/matlabwavesimulation/"},{"categories":["Methods","Code"],"content":"理论依据 详情参考这两篇论文： [1] 刘素美. 波浪数值模拟[J]. 科技与创新, 2018(13):132-133. [2] 赵珂,李茂华,郑建丽,田冠楠. 基于波浪谱的三维随机波浪数值模拟及仿真[J]. 舰船科学技术, 2014,36(02):37-39. 简单来说就是，波浪的模拟，可以由不同方向角、不同频率的很多个波，用随机的初始相位初始化后叠加得到。显然，组成波浪的波的频率个数越多、方向角个数越多，能够形成的波就更复杂（直观上也更真实）。 先给出波面模型的公式（根据文献[1]） $$ z = \\eta(x, y, t)=\\sum_{i=1}^{M} \\sum_{j=1}^{N} \\zeta_{i j} \\cos [{k}_{i}\\left(x \\cos \\alpha_{d j}+y \\sin \\alpha_{d j}\\right)-\\omega_{di}t+\\beta_{ij} $$ 其中$\\zeta_{ij},k_{i},\\alpha_{dj},\\omega_{di},\\beta_{ij}$分别为波浪的波幅、波数、方向角、频率和相位角。且$k_{i} = \\omega_{di}^{2}/g$。 可以看出，其中$x,y,t$是需要输入的参数，即坐标位置和时间，$z$就是波面高度，由于波的形成需要如上的这些参数。其中方向角和频率是需要进行划分的，其余的参数全都可以由不同的方向角和频率计算得到，下面讲波浪参数的确定。 ","date":"2020-02-17","objectID":"/matlabwavesimulation/:1:0","tags":["船舶与海洋工程","数值模拟"],"title":"Matlab波浪数值模拟","uri":"/matlabwavesimulation/"},{"categories":["Methods","Code"],"content":"方向角划分和选取 对传播方向角$\\alpha$进行划分时，设方向角的变化范围为主波向$\\alpha_{main}$两侧$[-\\pi/2,\\pi/2]$的范围，将此区域$N$等分，每一份的宽度为$d\\alpha=\\pi/N$（论文这里写的有误，请大家注意），选取每段的中心值作为方向角$\\alpha_{dj}$。 ","date":"2020-02-17","objectID":"/matlabwavesimulation/:1:1","tags":["船舶与海洋工程","数值模拟"],"title":"Matlab波浪数值模拟","uri":"/matlabwavesimulation/"},{"categories":["Methods","Code"],"content":"频率的划分和选取 论文中给定了频率选取区间的计算方式，这里不做描述，直接关注频率的划分公式 $$\\omega_{i}=\\left[\\frac{3.11}{H_{1 / 3}^{2} \\ln (M+2 / i)}\\right]^{1 / 4}$$ $$\\omega_{di}=\\frac{\\omega_{i+1}+\\omega_{i}}{2}$$ 这里采用等分能量法分割频率，将频率划分为$M$个等能量的区间，$\\omega_{i}$是各区间的分界频率，$\\omega_{di}$是各频率区间的中心值作为选取的频率。 备注：原文中第一个公式中是$M/i$，因为i的取值只能是从1~M-1，所以只能产生M-2个区间，这里坐了一下修改，保证M的个数与选取的频率个数相同 ","date":"2020-02-17","objectID":"/matlabwavesimulation/:1:2","tags":["船舶与海洋工程","数值模拟"],"title":"Matlab波浪数值模拟","uri":"/matlabwavesimulation/"},{"categories":["Methods","Code"],"content":"利用三维随机波浪谱确定波幅$\\zeta_{ij}$ 标准波浪谱为PM谱 $$S_{PM}(\\omega)=\\frac{0.78}{\\omega^{5}}exp[-\\frac{3.11}{\\omega^{4}H_{1/3}^{2}}]$$ 其中$H_{1/3}=0.0214v^{2}$为有义波高，$v$是海面风速，会影响波浪高度。由于PM 谱描述的是能量随频率的变化，而对于三维随机波浪，其能量分布与频率和方向角都有关，并且认为频率和方向角的影响相互独立，则引入只与方向角$\\alpha$有关的方向扩展谱函数$D_{f}(\\alpha)$ $$D_{f}(\\alpha)=\\frac{2}{\\pi} \\cos ^{2} \\alpha,\\left(-\\frac{\\pi}{2} \\leqslant \\alpha \\leqslant \\frac{\\pi}{2}\\right)$$ 最终得到三位随机波浪的方向波谱： $$S_{3D}(\\omega,\\alpha)=S_{PM}(\\omega)D_{f}(alpha)$$ 将划分好的频率和方向角代入下式，即可得到每个单元组成波的波幅$\\zeta_{ij}$ $$\\zeta_{ij}=\\sqrt{2S(\\omega,\\alpha)d{\\omega}d{\\alpha}}$$ 备注：论文这里公式有误，我已经做了修改 ","date":"2020-02-17","objectID":"/matlabwavesimulation/:1:3","tags":["船舶与海洋工程","数值模拟"],"title":"Matlab波浪数值模拟","uri":"/matlabwavesimulation/"},{"categories":["Methods","Code"],"content":"随机初始相位$\\beta_{ij}$的生成 文中写的是线性乘同余法，其实用简单的$0-2\\pi$的均匀分布随机采样就可以。 ","date":"2020-02-17","objectID":"/matlabwavesimulation/:1:4","tags":["船舶与海洋工程","数值模拟"],"title":"Matlab波浪数值模拟","uri":"/matlabwavesimulation/"},{"categories":["Methods","Code"],"content":"基本实现的静态波浪生成代码 n = 64; map = zeros(n,n); M = 1; % ferequence number N = 50; beta = 2*pi*rand(M,N); for x = 1:n for y=1:n map(x,y) = bo(x,y,M,N,beta); end end XX = 1:n; YY = 1:n; surf(XX, YY, map); axis([-5, n+5, -5, n+5, -5, 5]) functionH =bo(x,y,M,N,beta)t = 0; v = 5; g = 9.8; H_value = 0.0214*v^2; alpha_main = 0; da = pi/N; a = alpha_main-pi/2+da/2 : da : alpha_main+pi/2-da/2; wi = (3.11./(H_value^2*log((M+2)./(1:M+1)))).^(1/4); % wi = zeros(M+1,1); % for i = 1:M+1 % wi(i) = (3.11/(H_value^2*log((M+2)/i)))^(1/4); % end w = (wi(2:end)+wi(1:end-1))/2; dw = wi(2:end)-wi(1:end-1); % w = zeros(M,1); % dw = zeros(M,1); % for i = 1:M % w(i) = (wi(i+1)+wi(i))/2; % dw(i) = wi(i+1)-wi(i); % end H = 0; for i=1:M for j=1:N adj = a(j); wdi = w(i); Spm_w = 0.78*exp(-3.11/(wdi^4*H_value^2))/wdi^5; Df_alpha = 2*(cos(adj)^2)/pi; S3d = Spm_w*Df_alpha; A = sqrt(2*S3d*dw(i)*da); ki = wdi^2/g; H = H + A*cos(ki*(x*cos(adj)+y*sin(adj))-wdi*t+beta(i,j)); end end end 静态波浪静态波浪 \"\r静态波浪\r ","date":"2020-02-17","objectID":"/matlabwavesimulation/:2:0","tags":["船舶与海洋工程","数值模拟"],"title":"Matlab波浪数值模拟","uri":"/matlabwavesimulation/"},{"categories":["Methods","Code"],"content":"波浪的动态实现 （计算已改为矩阵并行化，为了提升实时的渲染速度） n = 100; t = 1; v = 8; M = 15; N = 15; g = 9.8; H_value = 0.0214*v^2; alpha_main = 0; beta = 2*pi*rand(M,N); da = pi/N; a = alpha_main-pi/2+da/2 : da : alpha_main+pi/2-da/2; wi = (3.11./(H_value^2*log((M+2)./(1:M+1)))).^(1/4); % wi = zeros(M+1,1); % for i = 1:M+1 % wi(i) = (3.11/(H_value^2*log((M+2)/i)))^(1/4); % end w = (wi(2:end)+wi(1:end-1))/2; dw = wi(2:end)-wi(1:end-1); % w = zeros(M,1); % dw = zeros(M,1); % for i = 1:M % w(i) = (wi(i+1)+wi(i))/2; % dw(i) = wi(i+1)-wi(i); % end Spm_w = 0.78*exp(-3.11./(w.^4*H_value^2))./w.^5; Df_alpha = 2*(cos(a).^2)/pi; S3d = Spm_w'*Df_alpha; A = sqrt(2*S3d.*(dw'*da)); ki = w.^2/g; A = repmat(A(:)', n*n, 1); aj = repmat(a, M, 1); a = aj(:)'； ki = ki'; ki = repmat(ki, 1, N); k = ki(:)'; extend_w = repmat(w',1,N); extend_w = repmat(extend_w(:)', n*n, 1); beta = repmat(beta(:)', n*n, 1); XX = 1:n; YY = 1:n; [X,Y] = meshgrid(XX, YY); t0 = 0; dt = 0.1； T = 100; X = reshape(X, n*n, 1); Y = reshape(Y, n*n, 1); clf shg set(gcf); MAP = wave2(X,Y,A,k,a,extend_w,t0,beta); MAP = reshape(MAP, n, n); surfplot = surf(XX, YY, MAP); %shading interp axis([-5,n+5,-5,n+5,-10,10]) for t = 0:dt:T MAP = wave2(X,Y,A,k,a,extend_w,t,beta); MAP = reshape(MAP, n, n); set(surfplot,'zdata',MAP); % shading interp drawnow end functionwave =wave2(X,Y,A,k,a,extend_w,t,beta)wave = sum(cos((X*cos(a)+Y*sin(a)).*k-extend_w.*t+beta).*A,2); end 动态波浪动态波浪 \"\r动态波浪\r ","date":"2020-02-17","objectID":"/matlabwavesimulation/:3:0","tags":["船舶与海洋工程","数值模拟"],"title":"Matlab波浪数值模拟","uri":"/matlabwavesimulation/"},{"categories":["Methods"],"content":"1. Adversarial nets 为了从data $X$ 中学习generator的分布 $p_g$ ，首先定义一个先验的噪声变量分布 $p_{\\boldsymbol{z}}(\\boldsymbol{z})$，然后用 $G\\left(\\boldsymbol{z} ; \\theta_{g}\\right)$ 表示从 $z$ 到 $x$ 空间的映射，$G$ 是由多层感知机表示的可微函数。然后我们再定义一个多层感知机 $D\\left(\\boldsymbol{x} ; \\theta_{d}\\right)$ 输出一个数值，这个数值表示 $x$ 来自data的概率（而不是 $p_g$ ）。我们训练 $D$ 去最大化给训练样本和从 $G$ 中采样生成的样本分配正确标签的概率。同时训练 $G$ 去最小化 $\\log (1-D(G(z)))$。 换言之，$D$ 和 $G$ 在玩一个minimax game： $$\\min_{G} \\max_{D} V(D, G)=\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text { data }}(\\boldsymbol{x})}[\\log D(\\boldsymbol{x})]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}(\\boldsymbol{z})}[\\log (1-D(G(\\boldsymbol{z})))]$$ See Figure 1 for a less formal, more pedagogical explanation of the approach. ExplanationExplanation \"\rExplanation\r ","date":"2019-07-23","objectID":"/gan/:1:0","tags":["GAN","Generative model"],"title":"GAN","uri":"/gan/"},{"categories":["Methods"],"content":"2. Theoretical Results Training approachTraining approach \"\rTraining approach\r ","date":"2019-07-23","objectID":"/gan/:2:0","tags":["GAN","Generative model"],"title":"GAN","uri":"/gan/"},{"categories":["Methods"],"content":"2.1 Global Optimality of $p_{g}=p_{\\text { data }}$ 我们首先考虑，对于任意的generator $G$ 去优化discriminator $D$ Proposition 1. 对于给定的 $G$，最优的 $D$ 是 $$D_{G}^{*}(\\boldsymbol{x})=\\frac{p_{\\text {data}}(\\boldsymbol{x})}{p_{\\text {data}}(\\boldsymbol{x})+p_{g}(\\boldsymbol{x})}$$ 注意，针对D的训练目标可以解释为最大化条件概率 $P(Y=y | \\boldsymbol{x})$ 的最大似然函数。 则minimax game可以重新表示为： $$\\begin{aligned} C(G) \u0026 = \\max_{D} V(G, D) \\\\ \u0026 =\\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text { data }}}\\left[\\log D_{G}^{*}(\\boldsymbol{x})\\right]+\\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}}\\left[\\log \\left(1-D_{G}^{*}(G(\\boldsymbol{z}))\\right)\\right] \\\\ \u0026 = \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text { data }}}\\left[\\log D_{G}^{*}(\\boldsymbol{x})\\right]+\\mathbb{E}_{\\boldsymbol{x} \\sim p_{g}}\\left[\\log \\left(1-D_{G}^{*}(\\boldsymbol{x})\\right)\\right] \\\\ \u0026 = \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text { data }}}\\left[\\log \\frac{p_{\\text { data }}(\\boldsymbol{x})}{P_{\\text { data }}(\\boldsymbol{x})+p_{g}(\\boldsymbol{x})}\\right]+\\mathbb{E}_{\\boldsymbol{x} \\sim p_{g}}\\left[\\log \\frac{p_{g}(\\boldsymbol{x})}{p_{\\text { data }}(\\boldsymbol{x})+p_{g}(\\boldsymbol{x})}\\right] \\end{aligned}$$ Theorem 1. 当且仅当$p_{g}=p_{\\text { data }}$时，C(G)取得全局最小值。最小值为-log4。 $$ C(G)=-\\log (4)+K L\\left(p_{\\text { data }} | \\frac{p_{\\text { data }}+p_{g}}{2}\\right)+K L\\left(p_{g} | \\frac{p_{\\text { data }}+p_{g}}{2}\\right) $$ $$ C(G)=-\\log (4)+2 \\cdot J S D\\left(p_{\\mathrm{data}} | p_{g}\\right) $$ ","date":"2019-07-23","objectID":"/gan/:2:1","tags":["GAN","Generative model"],"title":"GAN","uri":"/gan/"},{"categories":["Methods"],"content":"2.2 Convergence of Algorithm 1 Proposition 2. 如果G和D的能力足够强，在algorithm 1的每一步，D对于每一个给定的G给出最优解，并且$p_g$以提升如下的目标去更新参数： $$\\begin{aligned} C(G) \u0026 = \\max_{D} V(G, D) \\\\ \u0026 \\mathbb{E}_{\\boldsymbol{x} \\sim p_{\\text { data }}}\\left[\\log D_{G}^{*}(\\boldsymbol{x})\\right]+\\mathbb{E}_{\\boldsymbol{x} \\sim p_{g}}\\left[\\log \\left(1-D_{G}^{*}(\\boldsymbol{x})\\right)\\right] \\end{aligned}$$ 那么，$p_g$将收敛于$p_{data}$。 ","date":"2019-07-23","objectID":"/gan/:2:2","tags":["GAN","Generative model"],"title":"GAN","uri":"/gan/"},{"categories":["Methods"],"content":"3. Conclusion Challenges in generative modelingChallenges in generative modeling \"\rChallenges in generative modeling\r Advantages and disadvantages Disdvantages $p_g(x)$没有准确的表示 在训练期间，$D$必须与$G$同步（尤其是，在不更新$D$的情况下$G$不能训练太多次） Advantages 不需要Markov chain了，只有通过backprop获得梯度 学习过程不需要推理 可以将更广泛的函数合并到模型中 Adversarial模型也从生成网络中获得了一些统计优势（不是用数据样本直接更新参数，而是使用流经discriminator的梯度），这意味着输入的组成不是直接复制generator的参数；另一个优势是，它可以表征非常尖锐，甚至退化的分布，而基于马尔可夫链的方法要求分布比较模糊，以便链条能够在模式之间混合 ","date":"2019-07-23","objectID":"/gan/:3:0","tags":["GAN","Generative model"],"title":"GAN","uri":"/gan/"},{"categories":["Methods"],"content":"Referance Goodfellow I, Pouget-Abadie J, Mirza M, et al. Generative adversarial networks[J]. Communications of the ACM, 2020, 63(11): 139-144. Goodfellow I. Nips 2016 tutorial: Generative adversarial networks[J]. arXiv preprint arXiv:1701.00160, 2016. ","date":"2019-07-23","objectID":"/gan/:4:0","tags":["GAN","Generative model"],"title":"GAN","uri":"/gan/"}]